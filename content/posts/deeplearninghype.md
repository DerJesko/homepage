---
title: "Deeplearning Hype"
date: 2019-02-06T22:55:07+01:00
---
Today I listened to a talk of a person about her work and didn't understand the reason for it at all.
She wanted to build a crawler which can use object detection in images of websites to find a login form.
The idea is not bad however the ML algorithm(a neural network) was trained on a set of images and login forms which where generated by a traditional web crawler.
A machine learning algorithm only is as good as the data it knows which means this deep learning crawler will always be worse than the traditional crawler she used to generate the data.
But in addition to that it will be way slower.
What is the purpose of this crawler?
It seems to be such a case of just trying to follow the hype of machine learning without it being a good idea.
I can see the point of using neural networks as heuristics or shortcuts for harder (in this case I mean more runtime) problems e.g. smoke simulation (I think I heard about something like this) or a heuristic in playing go but this just doesn't fit in my mind.

